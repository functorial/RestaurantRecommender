{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Model1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing recommender systems is less intuitive than testing other predictive models. There are many common metrics to use, which can be found [here in this useful Medium article](https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832). The idea of these methods are all similar: generate a list of top ranked items to show the user, and your score is based on how many of the items you recommend are relevant to the user. The word \"relevant\" is not well-defined, but normally means that the user has interacted with this item in the past. \r\n",
    "\r\n",
    "In our case, our items are the restaurants and the interactions are orders. One issue with our data is that between 30-40% of our users have only ordered from a single restaurant, which makes the above methods tricky/impossible to apply.\r\n",
    "\r\n",
    "Recall that our model takes in a user's past five (5) orders as inputs together with one restaurant R and tries to predict how likely the user will order from R next. We will evaluate our model as follows. Given a user's sequence of five restaurants, and given the target restaurant R which is next in the sequence, we use the model to generate a ranked list of $k=5$ more restaurants that it thinks is the most likely to be ordered from next. If R is among the $k$ generated restaurants, we add $+1$ to the score, otherwise $+0$. \r\n",
    "\r\n",
    "Since there are only $100$ restaurants to recommend from, it should be very feasible to simply score each one, sort the rankings, and slice the top 5 as recommendations. This is a very privillaged position we are in since many recommender systems are deployed in a context where there are millions of items to recommend from. In that case, one could perform clustering based on the customer and vendor embeddings before ranking within the clusters.  \r\n",
    "\r\n",
    "Each user in the test set may possibly geenerate many length $5+1$ sequences, and we shall use all of these during the evaluation process. In the deployed version of the model, we could simply give recommendations based on the last $5$ orders made by the user.\r\n",
    "\r\n",
    "Baselines:\r\n",
    "1) Recommend the $5$ most popular restaurants.\r\n",
    "2) Given a sequence of $5$ orders, $m$ of which are unique and not null, recommend those restaurants together with the $5-m$ most popular restaurants. \r\n",
    "\r\n",
    "We will define 'popularity' of a vendor to be equal to its number of orders in the training data. Note that the unordered set of top five most popular vendors would be the same if we changed the definition of popularity of a vendor to be equal to its number of unique customers (although their rankings are slightly different, see the last cell of \"Munging.ipynb\"). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "import torch\r\n",
    "from PreprocessingHelpers import CustomDataset\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from Models.Models import Model1, Model2, Model3\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open(\"ProcessedData/test_sequences_padded_dataset.pkl\", \"rb\") as file:\r\n",
    "    test_sequences_padded_dataset = pickle.load(file)\r\n",
    "\r\n",
    "test_loader = DataLoader(test_sequences_padded_dataset, batch_size=1)\r\n",
    "num_trials = test_sequences_padded_dataset.vendor.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open(\"ProcessedData/vendors_tensor.pkl\", \"rb\") as file:\r\n",
    "    vendors_tensor = pickle.load(file)\r\n",
    "\r\n",
    "model1 = Model1(vendors=vendors_tensor, d_fc=64)\r\n",
    "model2 = Model2(vendors=vendors_tensor, d_fc=64)\r\n",
    "model3 = Model3(vendors=vendors_tensor, d_fc=64)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open(\"ProcessedData/popular_vendors.pkl\", \"rb\") as file:\r\n",
    "    popular_vendors = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Scoring for Model & Baselines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "popular_vendors.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "id\n",
       "28    3237\n",
       "25    2717\n",
       "14    2681\n",
       "19    2453\n",
       "18    2184\n",
       "15    2159\n",
       "75    1377\n",
       "21    1274\n",
       "36    1145\n",
       "6     1078\n",
       "Name: num_orders, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "k_most_popular = popular_vendors[:5].index.tolist()\r\n",
    "k_most_popular"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[28, 25, 14, 19, 18]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def baseline1_scoring(target:int, k_most_popular:list):\r\n",
    "    if target in k_most_popular:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def baseline2_scoring(seq:list, target:int, k_most_popular:list):\r\n",
    "    seq = list(set(seq))    # remove duplicates\r\n",
    "    try:\r\n",
    "        seq.remove(0)       # remove null-token\r\n",
    "    except ValueError:\r\n",
    "        pass\r\n",
    "    m = len(seq)\r\n",
    "    seq = seq + k_most_popular[:-m]\r\n",
    "    if target in seq:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def model_scoring(seq:torch.tensor, target:int, model, k:int=5):\r\n",
    "    seq = seq.view(1, -1)\r\n",
    "    y = torch.ones([100, 1], dtype=torch.long)      # 100 vendors\r\n",
    "    seq_dupe = y @ seq                              # 100 x 5 matrix\r\n",
    "    v_ids = torch.arange(start=1, end=101, dtype=torch.long)\r\n",
    "    rankings = model.forward(c_seq=seq_dupe, v_id=v_ids).view(-1)    \r\n",
    "    top_k = torch.topk(rankings, k)[1][:5]          # Essentially argmax for top k\r\n",
    "    top_k = top_k + 1                               # Shift indices to match vendors\r\n",
    "    if target in top_k:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Score baselines\r\n",
    "print(\"SCORING\\n=======================================================\")\r\n",
    "print(\"Random:\\t\\t2384 / 47677 = 5.00% \")\r\n",
    "baseline1_score = 0\r\n",
    "baseline2_score = 0\r\n",
    "for i, (c_seq, v_id) in enumerate(test_loader):\r\n",
    "    target = v_id.item()\r\n",
    "    c_seq_list = c_seq.view(-1).tolist()\r\n",
    "    baseline1_score += baseline1_scoring(target=target, k_most_popular=k_most_popular)\r\n",
    "    baseline2_score += baseline2_scoring(seq=c_seq_list, target=target, k_most_popular=k_most_popular)\r\n",
    "print(f'Baseline1:\\t{baseline1_score} / {num_trials} = {baseline1_score*100/num_trials:.2f}%')\r\n",
    "print(f'Baseline2:\\t{baseline2_score} / {num_trials} = {baseline2_score*100/num_trials:.2f}%')\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "# Score model1 at different epochs\r\n",
    "for epoch in range(0, 201, 10):\r\n",
    "    PATH = \"Models/model1_epoch\"+str(epoch)+\".pt\"\r\n",
    "    checkpoint = torch.load(PATH)\r\n",
    "    model1.load_state_dict(checkpoint['model_state_dict'])\r\n",
    "    model1.eval()\r\n",
    "    model1_score = 0\r\n",
    "    for i, (c_seq, v_id) in enumerate(test_loader):\r\n",
    "        target = v_id.item()\r\n",
    "        c_seq_list = c_seq.tolist()\r\n",
    "        with torch.no_grad():\r\n",
    "            model1_score += model_scoring(seq=c_seq, target=target, model=model1)\r\n",
    "    print(f'Model1_{epoch}:\\t{model1_score} / {num_trials} = {model1_score*100/num_trials:.2f}%')\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "# Score model2 at different epochs\r\n",
    "for epoch in range(0, 201, 10):\r\n",
    "    PATH = \"Models/model2_epoch\"+str(epoch)+\".pt\"\r\n",
    "    checkpoint = torch.load(PATH)\r\n",
    "    model2.load_state_dict(checkpoint['model_state_dict'])\r\n",
    "    model2.eval()\r\n",
    "    model2_score = 0\r\n",
    "    for i, (c_seq, v_id) in enumerate(test_loader):\r\n",
    "        target = v_id.item()\r\n",
    "        c_seq_list = c_seq.tolist()\r\n",
    "        with torch.no_grad():\r\n",
    "            model2_score += model_scoring(seq=c_seq, target=target, model=model2)\r\n",
    "    print(f'Model2_{epoch}:\\t{model2_score} / {num_trials} = {model2_score*100/num_trials:.2f}%')\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "# Score model3 at different epochs\r\n",
    "for epoch in range(0, 201, 10):\r\n",
    "    PATH = \"Models/model3_epoch\"+str(epoch)+\".pt\"\r\n",
    "    checkpoint = torch.load(PATH)\r\n",
    "    model3.load_state_dict(checkpoint['model_state_dict'])\r\n",
    "    model3.eval()\r\n",
    "    model3_score = 0\r\n",
    "    for i, (c_seq, v_id) in enumerate(test_loader):\r\n",
    "        target = v_id.item()\r\n",
    "        c_seq_list = c_seq.tolist()\r\n",
    "        with torch.no_grad():\r\n",
    "            model3_score += model_scoring(seq=c_seq, target=target, model=model3)\r\n",
    "    print(f'Model3_{epoch}:\\t{model3_score} / {num_trials} = {model3_score*100/num_trials:.2f}%')\r\n",
    "print(\"Done!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SCORING\n",
      "=======================================================\n",
      "Random:\t\t2384 / 47677 = 5.00% \n",
      "Baseline1:\t9233 / 47677 = 19.37%\n",
      "Baseline2:\t22134 / 47677 = 46.42%\n",
      "\n",
      "Model1_0:\t15579 / 47677 = 32.68%\n",
      "Model1_10:\t19627 / 47677 = 41.17%\n",
      "Model1_20:\t21271 / 47677 = 44.61%\n",
      "Model1_30:\t21525 / 47677 = 45.15%\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-eee999e41b31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mc_seq_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mmodel1_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Model1_{epoch}:\\t{model1_score} / {num_trials} = {model1_score*100/num_trials:.2f}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c63c65df21d1>\u001b[0m in \u001b[0;36mmodel_scoring\u001b[1;34m(seq, target, model, k)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mseq_dupe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mseq\u001b[0m                              \u001b[1;31m# 100 x 5 matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mv_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mrankings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseq_dupe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtop_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrankings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m          \u001b[1;31m# Essentially argmax for top k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtop_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m                               \u001b[1;31m# Shift indices to match vendors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexp\\Documents\\Programming\\KaggleProjects\\ResturauntRecommender\\Models\\Models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, c_seq, v_id)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mvendor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_cont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_misc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_ptag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_vtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mvendor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvendor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mvendor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvendor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;31m# feed through classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36melu\u001b[1;34m(input, alpha, inplace)\u001b[0m\n\u001b[0;32m   1210\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "PATH = \"Models/model3_epoch30.pt\"\r\n",
    "checkpoint = torch.load(PATH)\r\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\r\n",
    "model.eval()\r\n",
    "\r\n",
    "seq = iter(test_loader).next()[0]\r\n",
    "print(seq)\r\n",
    "seq = seq.view(1, -1)\r\n",
    "y = torch.ones([100, 1], dtype=torch.long)      # 100 vendors\r\n",
    "seq_dupe = y @ seq                              # 100 x 5 matrix\r\n",
    "v_ids = torch.arange(start=1, end=101, dtype=torch.long)\r\n",
    "rankings = model.forward(c_seq=seq_dupe, v_id=v_ids).view(-1)    \r\n",
    "top_k = torch.topk(rankings, 5)[1][:5]          # Essentially argmax for top k\r\n",
    "\r\n",
    "print(rankings.sort())\r\n",
    "print(len(rankings.tolist()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0, 0, 0, 0, 0]])\n",
      "torch.return_types.sort(\n",
      "values=tensor([-1.8947, -0.2750,  0.1777,  0.2211,  0.2352,  0.2771,  0.2915,  0.3192,\n",
      "         0.4320,  0.4535,  0.4745,  0.4770,  0.5204,  0.5537,  0.5568,  0.6537,\n",
      "         0.6652,  0.7893,  0.8370,  0.8842,  0.9073,  0.9184,  0.9256,  0.9333,\n",
      "         0.9869,  1.0183,  1.0595,  1.0597,  1.1827,  1.1938,  1.2413,  1.3262,\n",
      "         1.4613,  1.4752,  1.5227,  1.6915,  1.7520,  1.8084,  1.8130,  1.8162,\n",
      "         1.8978,  1.9081,  2.0173,  2.0688,  2.1732,  2.2013,  2.2482,  2.2524,\n",
      "         2.2655,  2.3086,  2.3252,  2.3374,  2.3715,  2.3715,  2.4481,  2.5075,\n",
      "         2.5914,  2.6870,  2.7724,  2.8197,  2.8339,  2.8449,  3.0061,  3.2087,\n",
      "         3.2184,  3.2267,  3.3115,  3.3319,  3.5376,  3.7694,  3.8482,  4.0569,\n",
      "         4.1448,  4.2146,  4.3211,  4.4387,  4.4860,  4.5340,  4.5407,  4.5507,\n",
      "         4.6005,  4.6885,  5.0842,  5.1168,  5.1998,  5.4003,  5.4051,  5.4095,\n",
      "         5.4135,  5.4139,  5.4508,  5.5101,  5.5362,  5.5765,  5.6087,  5.7044,\n",
      "         5.7361,  5.8321,  6.0721,  6.1134], grad_fn=<SortBackward>),\n",
      "indices=tensor([46, 24, 83, 36, 74, 20, 55, 57, 31, 65, 95, 80, 44, 18, 33,  7, 76, 19,\n",
      "        37, 78, 56, 68, 17, 30, 99, 21, 13, 73, 94, 54, 87, 79, 48, 51, 64, 28,\n",
      "        25, 92, 71, 90, 47, 34, 93, 96, 50,  4, 86, 45, 82, 97, 23,  6, 41,  9,\n",
      "        84, 14, 49, 75, 27, 98, 59, 32, 88, 89, 91, 85, 58,  8, 60, 10, 42, 61,\n",
      "        81, 12, 39, 43, 11, 26, 38, 15, 16,  3, 63, 72, 22, 40, 69, 29, 53,  2,\n",
      "        77, 62,  5, 52,  0, 35, 67, 70,  1, 66]))\n",
      "100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('deep': conda)"
  },
  "interpreter": {
   "hash": "bace64c8930513bc16c30bbfb6191e80e6b475bb1cc35e455b666e9fcbb27152"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}