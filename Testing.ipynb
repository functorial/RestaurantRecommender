{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing recommender systems is less intuitive than testing other predictive models. There are many common metrics to use, which can be found [here in this useful Medium article](https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832). The idea of these methods are all similar: generate a list of top ranked items to show the user, and your score is based on how many of the items you recommend are relevant to the user. The word \"relevant\" is not well-defined, but normally means that the user has interacted with this item in the past. \r\n",
    "\r\n",
    "In our case, our items are the restaurants and the interactions are orders. One issue with our data is that a plurality of our users have only ordered from a single restaurant, which makes the above methods tricky/impossible to apply. This is because the user's single order is used as the target, and hence they will be considered to have no interactions. \r\n",
    "\r\n",
    "Recall that our model takes in a user's past five ($k=5$) orders as inputs together with one restaurant $R$ and tries to predict how likely the user will order from $R$ next. We will evaluate our model as follows. Given a customer sequence of vendors, and given the target restaurant $R$ which is next in the sequence, we use the model to generate a ranked list of $k=5$ more restaurants that it thinks is the most likely to be ordered from next. If R is among the $k$ generated restaurants, we add $+1$ to the score, otherwise $+0$. \r\n",
    "\r\n",
    "Since there are only $100$ restaurants to recommend from, it should be very feasible to simply score each one, sort the rankings, and slice the top $5$ as recommendations. This is a very privillaged position we are in since many recommender systems are deployed in a context where there are millions of items to recommend from. In that case, one could perform clustering based on the customer and vendor embeddings before ranking within the clusters.  \r\n",
    "\r\n",
    "Each user in the test set may possibly geenerate many length $5+1$ sequences, and we shall use all of these during the evaluation process. In the deployed version of the model, we could simply give recommendations based on the last $5$ orders made by the user.\r\n",
    "\r\n",
    "Baselines:\r\n",
    "1) Recommend the $5$ most popular restaurants.\r\n",
    "2) Given a sequence of $5$ orders, $m$ of which are unique and not null, recommend those restaurants together with the $5-m$ most popular restaurants. \r\n",
    "\r\n",
    "We will define 'popularity' of a vendor to be equal to its number of orders in the training data. Note that the unordered set of top five most popular vendors would be the same if we changed the definition of popularity of a vendor to be equal to its number of unique customers (although their rankings are slightly different, see the last cell of \"Munging.ipynb\"). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "import torch\r\n",
    "from PreprocessingHelpers import CustomDataset\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from Models.Models import Model1, Model2, Model3, Model4, Model5, Model6, Model7, Model8, Model9\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open(\"ProcessedData/test_sequences_padded_dataset_6.pkl\", \"rb\") as file:\r\n",
    "    test_sequences_padded_dataset_6 = pickle.load(file)\r\n",
    "\r\n",
    "test_loader_6 = DataLoader(test_sequences_padded_dataset_6, batch_size=1, shuffle=True)\r\n",
    "num_trials_6 = test_sequences_padded_dataset_6.vendor.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open(\"ProcessedData/vendors_tensor.pkl\", \"rb\") as file:\r\n",
    "    vendors_tensor = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open(\"ProcessedData/popular_vendors.pkl\", \"rb\") as file:\r\n",
    "    popular_vendors = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Scoring for Model & Baselines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "popular_vendors.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "id\n",
       "28    3237\n",
       "25    2717\n",
       "14    2681\n",
       "19    2453\n",
       "18    2184\n",
       "15    2159\n",
       "75    1377\n",
       "21    1274\n",
       "36    1145\n",
       "6     1078\n",
       "Name: num_orders, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_most_popular(vendors, k):\r\n",
    "    return vendors[:k].index.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "most_popular_5 = get_most_popular(popular_vendors, 5)\r\n",
    "most_popular_5"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[28, 25, 14, 19, 18]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def baseline1_scoring(target:int, k_most_popular:list):\r\n",
    "    if target in k_most_popular:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def baseline2_scoring(seq:list, target:int, k_most_popular:list):\r\n",
    "    seq = list(set(seq))    # remove duplicates\r\n",
    "    try:\r\n",
    "        seq.remove(0)       # remove null-token\r\n",
    "    except ValueError:\r\n",
    "        pass\r\n",
    "    m = len(seq)\r\n",
    "    seq = seq + k_most_popular[:-m] if (m != 0) else k_most_popular\r\n",
    "    if target in seq:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def model_topk(seq:torch.tensor, model, k:int=5):\r\n",
    "    seq = seq.view(1, -1)\r\n",
    "    y = torch.ones([100, 1], dtype=torch.long)      # 100 vendors\r\n",
    "    seq_dupe = y @ seq                              # 100 x 5 matrix\r\n",
    "    v_ids = torch.arange(start=1, end=101, dtype=torch.long)\r\n",
    "    rankings = model.forward(c_seq=seq_dupe, v_id=v_ids).view(-1)    \r\n",
    "    top_k = torch.topk(rankings, k)[1][:5]          # Essentially argmax for top k\r\n",
    "    top_k = top_k + 1                               # Shift indices to match vendors\r\n",
    "    return top_k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def model_scoring(seq:torch.tensor, target:torch.tensor, model, k:int=5):\r\n",
    "    top_k = model_topk(seq, model, k)\r\n",
    "    if target.item() in top_k:\r\n",
    "        return 1\r\n",
    "    else:\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def load_score_model(model, base_path, epochs=201, test_every=20, test_loader=test_loader_6, num_trials=num_trials_6):\r\n",
    "    for epoch in range(0, epochs, test_every):\r\n",
    "        PATH = base_path + str(epoch) + \".pt\"\r\n",
    "        checkpoint = torch.load(PATH)\r\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\r\n",
    "        model.eval()\r\n",
    "        model_score = 0\r\n",
    "        for i, (c_seq, v_id) in enumerate(test_loader):\r\n",
    "            with torch.no_grad():\r\n",
    "                model_score += model_scoring(seq=c_seq, target=v_id, model=model, k=5)\r\n",
    "        print(f'Model_{epoch}:\\t{model_score} / {num_trials} = {model_score*100/num_trials:.2f}%')\r\n",
    "    print(\"Done!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Give 5 Recommendations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baselines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Score baselines\r\n",
    "\r\n",
    "print(\"SCORING\\n=======================================================\")\r\n",
    "print(\"Random:\\t\\t2384 / 47677 = 5.00% \")\r\n",
    "baseline1_score = 0\r\n",
    "baseline2_score = 0\r\n",
    "for i, (c_seq, v_id) in enumerate(test_loader_6):\r\n",
    "    target = v_id.item()\r\n",
    "    c_seq_list = c_seq.view(-1).tolist()\r\n",
    "    baseline1_score += baseline1_scoring(target=target, k_most_popular=most_popular_5)\r\n",
    "    baseline2_score += baseline2_scoring(seq=c_seq_list, target=target, k_most_popular=most_popular_5)\r\n",
    "print(f'Baseline1:\\t{baseline1_score} / {num_trials_6} = {baseline1_score*100/num_trials_6:.2f}%')\r\n",
    "print(f'Baseline2:\\t{baseline2_score} / {num_trials_6} = {baseline2_score*100/num_trials_6:.2f}%')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SCORING\n",
      "=======================================================\n",
      "Random:\t\t2384 / 47677 = 5.00% \n",
      "Baseline1:\t9233 / 47677 = 19.37%\n",
      "Baseline2:\t24204 / 47677 = 50.77%\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model1_64"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Score model1_64 at different epochs\r\n",
    "\r\n",
    "model = Model1(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adam_no_scheduler/model1_64_epoch\"\r\n",
    "load_score_model(model, base_path, test_every=40)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model1_64_0:\t15579 / 47677 = 32.68%\n",
      "Model1_64_40:\t21454 / 47677 = 45.00%\n",
      "Model1_64_80:\t22135 / 47677 = 46.43%\n",
      "Model1_64_120:\t22525 / 47677 = 47.25%\n",
      "Model1_64_160:\t22844 / 47677 = 47.91%\n",
      "Model1_64_200:\t22797 / 47677 = 47.82%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model1_128"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Score model1_128 at different epochs\r\n",
    "\r\n",
    "model1 = Model1(vendors=vendors_tensor, d_fc=128)\r\n",
    "base_path = \"Models/give_5/adam_no_scheduler/model1_128_epoch\"\r\n",
    "\r\n",
    "load_score_model(model, base_path, test_every=40)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model1_0:\t15844 / 47677 = 33.23%\n",
      "Model1_40:\t21924 / 47677 = 45.98%\n",
      "Model1_80:\t22116 / 47677 = 46.39%\n",
      "Model1_120:\t22375 / 47677 = 46.93%\n",
      "Model1_160:\t22801 / 47677 = 47.82%\n",
      "Model1_200:\t22547 / 47677 = 47.29%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model1_64s"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Score model1_64s at different epochs\r\n",
    "\r\n",
    "model1 = Model1(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model1_64s_epoch\"\r\n",
    "load_score_model(model, base_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model1_64s_0:\t15644 / 47677 = 32.81%\n",
      "Model1_64s_20:\t21455 / 47677 = 45.00%\n",
      "Model1_64s_40:\t21628 / 47677 = 45.36%\n",
      "Model1_64s_60:\t22108 / 47677 = 46.37%\n",
      "Model1_64s_80:\t22378 / 47677 = 46.94%\n",
      "Model1_64s_100:\t22836 / 47677 = 47.90%\n",
      "Model1_64s_120:\t22922 / 47677 = 48.08%\n",
      "Model1_64s_140:\t22967 / 47677 = 48.17%\n",
      "Model1_64s_160:\t23027 / 47677 = 48.30%\n",
      "Model1_64s_180:\t23044 / 47677 = 48.33%\n",
      "Model1_64s_200:\t23041 / 47677 = 48.33%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Score model2s at different epochs\r\n",
    "\r\n",
    "model = Model2(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model2s_epoch\"\r\n",
    "load_score_model(model, base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Score model3s at different epochs\r\n",
    "\r\n",
    "model = Model3(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model3s_epoch\"\r\n",
    "load_score_model(model, base_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model3_0:\t17206 / 47677 = 36.09%\n",
      "Model3_20:\t21731 / 47677 = 45.58%\n",
      "Model3_40:\t21939 / 47677 = 46.02%\n",
      "Model3_60:\t22209 / 47677 = 46.58%\n",
      "Model3_80:\t22862 / 47677 = 47.95%\n",
      "Model3_100:\t23002 / 47677 = 48.25%\n",
      "Model3_120:\t22947 / 47677 = 48.13%\n",
      "Model3_140:\t23112 / 47677 = 48.48%\n",
      "Model3_160:\t23155 / 47677 = 48.57%\n",
      "Model3_180:\t23144 / 47677 = 48.54%\n",
      "Model3_200:\t23141 / 47677 = 48.54%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Score model4 at different epochs\r\n",
    "\r\n",
    "model = Model4(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model4_64s_epoch\"\r\n",
    "load_score_model(model, base_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model4_0:\t15939 / 47677 = 33.43%\n",
      "Model4_20:\t20365 / 47677 = 42.71%\n",
      "Model4_40:\t21525 / 47677 = 45.15%\n",
      "Model4_60:\t21288 / 47677 = 44.65%\n",
      "Model4_80:\t21902 / 47677 = 45.94%\n",
      "Model4_100:\t22149 / 47677 = 46.46%\n",
      "Model4_120:\t22739 / 47677 = 47.69%\n",
      "Model4_140:\t22912 / 47677 = 48.06%\n",
      "Model4_160:\t22943 / 47677 = 48.12%\n",
      "Model4_180:\t23068 / 47677 = 48.38%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Score model5 at different epochs\r\n",
    "\r\n",
    "model = Model5(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model5_64s_epoch\"\r\n",
    "load_score_model(model, base_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model5_0:\t17857 / 47677 = 37.45%\n",
      "Model5_20:\t23909 / 47677 = 50.15%\n",
      "Model5_40:\t24169 / 47677 = 50.69%\n",
      "Model5_60:\t24585 / 47677 = 51.57%\n",
      "Model5_80:\t24437 / 47677 = 51.26%\n",
      "Model5_100:\t24934 / 47677 = 52.30%\n",
      "Model5_120:\t24973 / 47677 = 52.38%\n",
      "Model5_140:\t24989 / 47677 = 52.41%\n",
      "Model5_160:\t25039 / 47677 = 52.52%\n",
      "Model5_180:\t25000 / 47677 = 52.44%\n",
      "Model5_200:\t25059 / 47677 = 52.56%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model6"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Score model6 at different epochs\r\n",
    "\r\n",
    "model = Model6(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model6_64s_epoch\"\r\n",
    "load_score_model(model, base_path, epochs=301)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model6_0:\t16565 / 47677 = 34.74%\n",
      "Model6_20:\t23492 / 47677 = 49.27%\n",
      "Model6_40:\t24497 / 47677 = 51.38%\n",
      "Model6_60:\t24611 / 47677 = 51.62%\n",
      "Model6_80:\t24778 / 47677 = 51.97%\n",
      "Model6_100:\t25080 / 47677 = 52.60%\n",
      "Model6_120:\t25098 / 47677 = 52.64%\n",
      "Model6_140:\t25170 / 47677 = 52.79%\n",
      "Model6_160:\t25168 / 47677 = 52.79%\n",
      "Model6_180:\t25201 / 47677 = 52.86%\n",
      "Model6_200:\t25158 / 47677 = 52.77%\n",
      "Model6_220:\t25150 / 47677 = 52.75%\n",
      "Model6_240:\t25184 / 47677 = 52.82%\n",
      "Model6_260:\t25167 / 47677 = 52.79%\n",
      "Model6_280:\t25173 / 47677 = 52.80%\n",
      "Model6_300:\t25158 / 47677 = 52.77%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model7"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model7_64"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Score model7 at different epochs\r\n",
    "\r\n",
    "model = Model7(vendors=vendors_tensor, d_fc=64)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model7_64s_epoch\"\r\n",
    "load_score_model(model, base_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model7_0:\t16003 / 47677 = 33.57%\n",
      "Model7_20:\t25734 / 47677 = 53.98%\n",
      "Model7_40:\t25703 / 47677 = 53.91%\n",
      "Model7_60:\t26025 / 47677 = 54.59%\n",
      "Model7_80:\t25891 / 47677 = 54.31%\n",
      "Model7_100:\t26020 / 47677 = 54.58%\n",
      "Model7_120:\t26135 / 47677 = 54.82%\n",
      "Model7_140:\t26127 / 47677 = 54.80%\n",
      "Model7_160:\t26133 / 47677 = 54.81%\n",
      "Model7_180:\t26134 / 47677 = 54.81%\n",
      "Model7_200:\t26145 / 47677 = 54.84%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model7_128"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Score model7 at different epochs\r\n",
    "\r\n",
    "model = Model7(vendors=vendors_tensor, d_fc=128)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model7_128s_epoch\"\r\n",
    "load_score_model(model, base_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model7_0:\t17695 / 47677 = 37.11%\n",
      "Model7_20:\t25597 / 47677 = 53.69%\n",
      "Model7_40:\t26191 / 47677 = 54.93%\n",
      "Model7_60:\t25941 / 47677 = 54.41%\n",
      "Model7_80:\t26117 / 47677 = 54.78%\n",
      "Model7_100:\t25946 / 47677 = 54.42%\n",
      "Model7_120:\t25983 / 47677 = 54.50%\n",
      "Model7_140:\t26130 / 47677 = 54.81%\n",
      "Model7_160:\t26166 / 47677 = 54.88%\n",
      "Model7_180:\t26120 / 47677 = 54.79%\n",
      "Model7_200:\t26199 / 47677 = 54.95%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model7_256"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Score model7 at different epochs\r\n",
    "\r\n",
    "model = Model7(vendors=vendors_tensor, d_fc=256)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model7_256s_epoch\"\r\n",
    "load_score_model(model, base_path, test_every=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model1_64_0:\t16446 / 47677 = 34.49%\n",
      "Model1_64_10:\t25642 / 47677 = 53.78%\n",
      "Model1_64_20:\t25940 / 47677 = 54.41%\n",
      "Model1_64_30:\t25849 / 47677 = 54.22%\n",
      "Model1_64_40:\t25884 / 47677 = 54.29%\n",
      "Model1_64_50:\t26168 / 47677 = 54.89%\n",
      "Model1_64_60:\t25978 / 47677 = 54.49%\n",
      "Model1_64_70:\t26071 / 47677 = 54.68%\n",
      "Model1_64_80:\t26087 / 47677 = 54.72%\n",
      "Model1_64_90:\t26166 / 47677 = 54.88%\n",
      "Model1_64_100:\t26009 / 47677 = 54.55%\n",
      "Model1_64_110:\t26092 / 47677 = 54.73%\n",
      "Model1_64_120:\t25960 / 47677 = 54.45%\n",
      "Model1_64_130:\t26276 / 47677 = 55.11%\n",
      "Model1_64_140:\t26319 / 47677 = 55.20%\n",
      "Model1_64_150:\t26354 / 47677 = 55.28%\n",
      "Model1_64_160:\t26321 / 47677 = 55.21%\n",
      "Model1_64_170:\t26355 / 47677 = 55.28%\n",
      "Model1_64_180:\t26327 / 47677 = 55.22%\n",
      "Model1_64_190:\t26323 / 47677 = 55.21%\n",
      "Model1_64_200:\t26288 / 47677 = 55.14%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model8"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Score model8 at different epochs\r\n",
    "\r\n",
    "model = Model8(vendors=vendors_tensor, d_fc=256)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model8_256s_epoch\"\r\n",
    "load_score_model(model, base_path, test_every=10, epochs=300)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_0:\t17935 / 47677 = 37.62%\n",
      "Model_10:\t25130 / 47677 = 52.71%\n",
      "Model_20:\t26079 / 47677 = 54.70%\n",
      "Model_30:\t26113 / 47677 = 54.77%\n",
      "Model_40:\t26112 / 47677 = 54.77%\n",
      "Model_50:\t26166 / 47677 = 54.88%\n",
      "Model_60:\t26058 / 47677 = 54.66%\n",
      "Model_70:\t26257 / 47677 = 55.07%\n",
      "Model_80:\t26055 / 47677 = 54.65%\n",
      "Model_90:\t26071 / 47677 = 54.68%\n",
      "Model_100:\t26099 / 47677 = 54.74%\n",
      "Model_110:\t26063 / 47677 = 54.67%\n",
      "Model_120:\t26077 / 47677 = 54.70%\n",
      "Model_130:\t26052 / 47677 = 54.64%\n",
      "Model_140:\t26190 / 47677 = 54.93%\n",
      "Model_150:\t26066 / 47677 = 54.67%\n",
      "Model_160:\t26273 / 47677 = 55.11%\n",
      "Model_170:\t26299 / 47677 = 55.16%\n",
      "Model_180:\t26326 / 47677 = 55.22%\n",
      "Model_190:\t26348 / 47677 = 55.26%\n",
      "Model_200:\t26308 / 47677 = 55.18%\n",
      "Model_210:\t26306 / 47677 = 55.18%\n",
      "Model_220:\t26324 / 47677 = 55.21%\n",
      "Model_230:\t26332 / 47677 = 55.23%\n",
      "Model_240:\t26333 / 47677 = 55.23%\n",
      "Model_250:\t26334 / 47677 = 55.23%\n",
      "Model_260:\t26334 / 47677 = 55.23%\n",
      "Model_270:\t26334 / 47677 = 55.23%\n",
      "Model_280:\t26334 / 47677 = 55.23%\n",
      "Model_290:\t26334 / 47677 = 55.23%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model9"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Score model8 at different epochs\r\n",
    "\r\n",
    "model = Model9(vendors=vendors_tensor, d_fc=256)\r\n",
    "base_path = \"Models/give_5/adamw_scheduler/model9_256s_epoch\"\r\n",
    "load_score_model(model, base_path, test_every=20, epochs=200)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_0:\t17330 / 47677 = 36.35%\n",
      "Model_20:\t25648 / 47677 = 53.80%\n",
      "Model_40:\t25924 / 47677 = 54.37%\n",
      "Model_60:\t26018 / 47677 = 54.57%\n",
      "Model_80:\t26040 / 47677 = 54.62%\n",
      "Model_100:\t25886 / 47677 = 54.29%\n",
      "Model_120:\t26060 / 47677 = 54.66%\n",
      "Model_140:\t26294 / 47677 = 55.15%\n",
      "Model_160:\t26302 / 47677 = 55.17%\n",
      "Model_180:\t26328 / 47677 = 55.22%\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results Summary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print(\"SCORING\\n=======================================================\")\r\n",
    "print(\"Random:\\t\\t2384 / 47677 = 5.00% \")\r\n",
    "baseline1_score = 0\r\n",
    "baseline2_score = 0\r\n",
    "for i, (c_seq, v_id) in enumerate(test_loader_6):\r\n",
    "    target = v_id.item()\r\n",
    "    c_seq_list = c_seq.view(-1).tolist()\r\n",
    "    baseline1_score += baseline1_scoring(target=target, k_most_popular=most_popular_5)\r\n",
    "    baseline2_score += baseline2_scoring(seq=c_seq_list, target=target, k_most_popular=most_popular_5)\r\n",
    "print(f'Baseline1:\\t{baseline1_score} / {num_trials_6} = {baseline1_score*100/num_trials_6:.2f}%')\r\n",
    "print(f'Baseline2:\\t{baseline2_score} / {num_trials_6} = {baseline2_score*100/num_trials_6:.2f}%\\n')\r\n",
    "\r\n",
    "x = [(Model1, \"model1_64s_epoch200.pt\"), (Model2, \"model2s_epoch200.pt\"), (Model3, \"model3s_epoch200.pt\"), (Model4, \"model4_64s_epoch190.pt\"), (Model5, \"model5_64s_epoch200.pt\"), (Model6, \"model6_64s_epoch200.pt\"), (Model7, \"model7_256s_epoch200.pt\"), (Model8, \"model8_256s_epoch200.pt\"), (Model9, \"model9_256s_epoch200.pt\")]\r\n",
    "for j, p in enumerate(x):\r\n",
    "    Model, path = p[0], p[1]\r\n",
    "    test_model = Model(vendors=vendors_tensor)\r\n",
    "    PATH = \"Models/give_5/adamw_scheduler/\" + path\r\n",
    "    checkpoint = torch.load(PATH)\r\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\r\n",
    "    test_model.eval()\r\n",
    "    model_score = 0\r\n",
    "    for i, (c_seq, v_id) in enumerate(test_loader_6):\r\n",
    "        with torch.no_grad():\r\n",
    "            model_score += model_scoring(seq=c_seq, target=v_id, model=test_model, k=5)\r\n",
    "    print(f'Model{j+1}:\\t\\t{model_score} / {num_trials_6} = {model_score*100/num_trials_6:.2f}%')\r\n",
    "    print(f'\\t\\tNum parameters: {sum([p.numel() for p in test_model.parameters()])}')\r\n",
    "   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SCORING\n",
      "=======================================================\n",
      "Random:\t\t2384 / 47677 = 5.00% \n",
      "Baseline1:\t9233 / 47677 = 19.37%\n",
      "Baseline2:\t24204 / 47677 = 50.77%\n",
      "\n",
      "Model1:\t\t23041 / 47677 = 48.33%\n",
      "\t\tNum parameters: 17506\n",
      "Model2:\t\t16074 / 47677 = 33.71%\n",
      "\t\tNum parameters: 17506\n",
      "Model3:\t\t23141 / 47677 = 48.54%\n",
      "\t\tNum parameters: 16759\n",
      "Model4:\t\t23069 / 47677 = 48.39%\n",
      "\t\tNum parameters: 16887\n",
      "Model5:\t\t25059 / 47677 = 52.56%\n",
      "\t\tNum parameters: 28698\n",
      "Model6:\t\t25158 / 47677 = 52.77%\n",
      "\t\tNum parameters: 28698\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Examine Outputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Load best model for testing\r\n",
    "\r\n",
    "PATH = \"Models/give_5/adamw_scheduler/model7_256s_epoch150.pt\"\r\n",
    "checkpoint = torch.load(PATH)\r\n",
    "test_model = Model7(vendors_tensor, d_fc=256)\r\n",
    "test_model.load_state_dict(checkpoint['model_state_dict'])\r\n",
    "test_model.eval()\r\n",
    "\r\n",
    "test_loader_6 = DataLoader(test_sequences_padded_dataset_6, batch_size=1, shuffle=False)\r\n",
    "test_iter = iter(test_loader_6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Visualize model predictions against baseline predictions\r\n",
    "\r\n",
    "num_tests = 25\r\n",
    "print('Test\\tInputs\\t\\t\\tTarget\\tPreds (Ordered)\\t\\tModel\\tBase2\\n=================================================================================')\r\n",
    "for i in range(num_tests):\r\n",
    "    c_seq, v_id = test_iter.next()\r\n",
    "\r\n",
    "    m_preds = model_topk(c_seq, test_model)\r\n",
    "    m_right = \"X\" if v_id in m_preds == 1 else \" \"\r\n",
    "    \r\n",
    "    b_pred = baseline2_scoring(seq=c_seq.view(-1).tolist(), target=v_id, k_most_popular=most_popular_5)\r\n",
    "    b_right = \"X\" if b_pred == 1 else \" \"\r\n",
    "    print(f'{i}:\\t{c_seq.tolist()}\\t{v_id.item()}\\t{m_preds}\\t{m_right}\\t{b_right}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test\tInputs\t\t\tTarget\tPreds (Ordered)\t\tModel\tBase2\n",
      "=================================================================================\n",
      "0:\t[[0, 0, 0, 0, 0]]\t57\t[18, 14, 25, 28, 20]\t \t \n",
      "1:\t[[0, 0, 0, 0, 57]]\t57\t[18, 14, 25, 28, 20]\tX\tX\n",
      "2:\t[[0, 0, 0, 0, 0]]\t62\t[18, 14, 25, 28, 20]\t \t \n",
      "3:\t[[0, 0, 0, 0, 0]]\t74\t[18, 14, 25, 28, 20]\t \t \n",
      "4:\t[[0, 0, 0, 0, 0]]\t42\t[18, 14, 25, 28, 20]\t \t \n",
      "5:\t[[0, 0, 0, 0, 42]]\t60\t[18, 14, 25, 28, 20]\t \t \n",
      "6:\t[[0, 0, 0, 42, 60]]\t93\t[18, 14, 25, 28, 20]\t \t \n",
      "7:\t[[0, 0, 0, 0, 0]]\t38\t[18, 14, 25, 28, 20]\t \t \n",
      "8:\t[[0, 0, 0, 0, 38]]\t83\t[18, 14, 25, 28, 20]\tX\t \n",
      "9:\t[[0, 0, 0, 0, 0]]\t94\t[18, 14, 25, 28, 20]\t \t \n",
      "10:\t[[0, 0, 0, 0, 0]]\t58\t[18, 14, 25, 28, 20]\t \t \n",
      "11:\t[[0, 0, 0, 0, 0]]\t45\t[18, 14, 25, 28, 20]\t \t \n",
      "12:\t[[0, 0, 0, 0, 45]]\t25\t[18, 14, 25, 28, 20]\t \tX\n",
      "13:\t[[0, 0, 0, 45, 25]]\t85\t[18, 14, 25, 28, 20]\t \t \n",
      "14:\t[[0, 0, 45, 25, 85]]\t31\t[18, 14, 25, 28, 20]\t \t \n",
      "15:\t[[0, 45, 25, 85, 31]]\t31\t[18, 14, 25, 28, 20]\tX\tX\n",
      "16:\t[[45, 25, 85, 31, 31]]\t52\t[18, 14, 25, 28, 20]\t \t \n",
      "17:\t[[25, 85, 31, 31, 52]]\t31\t[18, 14, 25, 28, 20]\tX\tX\n",
      "18:\t[[85, 31, 31, 52, 31]]\t74\t[18, 14, 25, 28, 20]\t \t \n",
      "19:\t[[31, 31, 52, 31, 74]]\t74\t[18, 14, 25, 28, 20]\t \tX\n",
      "20:\t[[0, 0, 0, 0, 0]]\t7\t[18, 14, 25, 28, 20]\t \t \n",
      "21:\t[[0, 0, 0, 0, 0]]\t15\t[18, 14, 25, 28, 20]\t \t \n",
      "22:\t[[0, 0, 0, 0, 0]]\t14\t[18, 14, 25, 28, 20]\tX\tX\n",
      "23:\t[[0, 0, 0, 0, 0]]\t43\t[18, 14, 25, 28, 20]\t \t \n",
      "24:\t[[0, 0, 0, 0, 43]]\t82\t[18, 14, 25, 28, 20]\t \t \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# The model predicts popular vendors for first time customers (zero tensors)\r\n",
    "\r\n",
    "c_seq = torch.tensor([[0, 0, 0, 0, 0]])\r\n",
    "m_preds = model_topk(c_seq, test_model).tolist()\r\n",
    "\r\n",
    "print(f'Most popular:\\t{popular_vendors.head(5).index.tolist()}')\r\n",
    "print(f'Model(zero):\\t{m_preds}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most popular:\t[28, 25, 14, 19, 18]\n",
      "Model(zero):\t[18, 14, 25, 28, 20]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "test_loader_6 = DataLoader(test_sequences_padded_dataset_6, batch_size=1, shuffle=False)\r\n",
    "test_iter = iter(test_loader_6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Visualize model predictions against baseline predictions\r\n",
    "\r\n",
    "num_tests = 25\r\n",
    "print('Test\\tInputs\\t\\t\\tTarget\\tPreds (Ordered)\\t\\tModel\\tBase2\\n=================================================================================')\r\n",
    "for i in range(num_tests):\r\n",
    "    c_seq, v_id = test_iter.next()\r\n",
    "\r\n",
    "    m_pred = model_scoring(c_seq, v_id, test_model)\r\n",
    "    m_right = \"X\" if m_pred == 1 else \" \"\r\n",
    "    \r\n",
    "    b_pred = baseline2_scoring(seq=c_seq.view(-1).tolist(), target=v_id, k_most_popular=most_popular_5)\r\n",
    "    b_right = \"X\" if b_pred == 1 else \" \"\r\n",
    "    print(f'{i}:\\t{c_seq.tolist()}\\t{v_id.item()}\\t{m_preds}\\t{m_right}\\t{b_right}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test\tInputs\t\t\tTarget\tPreds (Ordered)\t\tModel\tBase2\n",
      "=================================================================================\n",
      "0:\t[[0, 0, 0, 0, 0]]\t57\t[18, 14, 25, 28, 20]\t \t \n",
      "1:\t[[0, 0, 0, 0, 57]]\t57\t[18, 14, 25, 28, 20]\tX\tX\n",
      "2:\t[[0, 0, 0, 0, 0]]\t62\t[18, 14, 25, 28, 20]\t \t \n",
      "3:\t[[0, 0, 0, 0, 0]]\t74\t[18, 14, 25, 28, 20]\t \t \n",
      "4:\t[[0, 0, 0, 0, 0]]\t42\t[18, 14, 25, 28, 20]\t \t \n",
      "5:\t[[0, 0, 0, 0, 42]]\t60\t[18, 14, 25, 28, 20]\t \t \n",
      "6:\t[[0, 0, 0, 42, 60]]\t93\t[18, 14, 25, 28, 20]\t \t \n",
      "7:\t[[0, 0, 0, 0, 0]]\t38\t[18, 14, 25, 28, 20]\t \t \n",
      "8:\t[[0, 0, 0, 0, 38]]\t83\t[18, 14, 25, 28, 20]\tX\t \n",
      "9:\t[[0, 0, 0, 0, 0]]\t94\t[18, 14, 25, 28, 20]\t \t \n",
      "10:\t[[0, 0, 0, 0, 0]]\t58\t[18, 14, 25, 28, 20]\t \t \n",
      "11:\t[[0, 0, 0, 0, 0]]\t45\t[18, 14, 25, 28, 20]\t \t \n",
      "12:\t[[0, 0, 0, 0, 45]]\t25\t[18, 14, 25, 28, 20]\t \tX\n",
      "13:\t[[0, 0, 0, 45, 25]]\t85\t[18, 14, 25, 28, 20]\t \t \n",
      "14:\t[[0, 0, 45, 25, 85]]\t31\t[18, 14, 25, 28, 20]\t \t \n",
      "15:\t[[0, 45, 25, 85, 31]]\t31\t[18, 14, 25, 28, 20]\tX\tX\n",
      "16:\t[[45, 25, 85, 31, 31]]\t52\t[18, 14, 25, 28, 20]\t \t \n",
      "17:\t[[25, 85, 31, 31, 52]]\t31\t[18, 14, 25, 28, 20]\tX\tX\n",
      "18:\t[[85, 31, 31, 52, 31]]\t74\t[18, 14, 25, 28, 20]\t \t \n",
      "19:\t[[31, 31, 52, 31, 74]]\t74\t[18, 14, 25, 28, 20]\t \tX\n",
      "20:\t[[0, 0, 0, 0, 0]]\t7\t[18, 14, 25, 28, 20]\t \t \n",
      "21:\t[[0, 0, 0, 0, 0]]\t15\t[18, 14, 25, 28, 20]\t \t \n",
      "22:\t[[0, 0, 0, 0, 0]]\t14\t[18, 14, 25, 28, 20]\tX\tX\n",
      "23:\t[[0, 0, 0, 0, 0]]\t43\t[18, 14, 25, 28, 20]\t \t \n",
      "24:\t[[0, 0, 0, 0, 43]]\t82\t[18, 14, 25, 28, 20]\t \t \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "test_iter = iter(test_loader_6)\r\n",
    "\r\n",
    "seq_len = 5\r\n",
    "hit = torch.zeros([3, seq_len+2], dtype=torch.long)     # Rows: base2_score, model_score, total\r\n",
    "                                                        # Cols: 0-5 num_orders, total\r\n",
    "\r\n",
    "for c_seq, v_id in tqdm(test_iter):\r\n",
    "    num_orders = torch.count_nonzero(c_seq).item()\r\n",
    "    \r\n",
    "    b_score = baseline2_scoring(seq=c_seq.view(-1).tolist(), target=v_id, k_most_popular=most_popular_5)\r\n",
    "    hit[0, num_orders] += b_score\r\n",
    "    hit[0, -1] += b_score\r\n",
    "    \r\n",
    "    m_score = model_scoring(c_seq, v_id, test_model)\r\n",
    "    hit[1, num_orders] += m_score\r\n",
    "    hit[1, -1] += m_score\r\n",
    "\r\n",
    "    hit[2, num_orders] += 1\r\n",
    "    hit[2, -1] += 1\r\n",
    "\r\n",
    "r0 = ''.join([f'\\t{hit[0,i]}' for i in range(seq_len+1)])\r\n",
    "r1 = ''.join([f'\\t{hit[1,i]}' for i in range(seq_len+1)])\r\n",
    "r2 = ''.join([f'\\t{hit[2,i]}' for i in range(seq_len+1)])\r\n",
    "r3 = ''.join([f'\\t{hit[0,i] / hit[2,i] * 100:.1f}%' for i in range(seq_len+1)])\r\n",
    "r4 = ''.join([f'\\t{hit[1,i] / hit[2,i] * 100:.1f}%' for i in range(seq_len+1)])\r\n",
    "r5 = ''.join([f'\\t{torch.sum(hit[0,i:]).item() / torch.sum(hit[2,i:]).item() * 100:.1f}%' for i in range(seq_len+1)])\r\n",
    "r6 = ''.join([f'\\t{torch.sum(hit[1,i:]).item() / torch.sum(hit[2,i:]).item() * 100:.1f}%' for i in range(seq_len+1)])\r\n",
    "\r\n",
    "print('\\t\\t\\t\\t# Orders == i')\r\n",
    "print('\\t\\t  0\\t  1\\t  2\\t  3\\t  4\\t  5')\r\n",
    "print('___________________________________________________________________')\r\n",
    "print('Base      |' + r0)\r\n",
    "print('Model     |' + r1)\r\n",
    "print('Total     |' + r2)\r\n",
    "print('___________________________________________________________________')\r\n",
    "print('=i B/T %     |' + r3)\r\n",
    "print('=i M/T %     |' + r4)\r\n",
    "print('___________________________________________________________________')\r\n",
    "print('>=i B/T % |' + r5)\r\n",
    "print('>=i M/T % |' + r6)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 47677/47677 [03:39<00:00, 216.94it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t\t\t\t# Orders == i\n",
      "\t\t  0\t  1\t  2\t  3\t  4\t  5\n",
      "___________________________________________________________________\n",
      "Base      |\t2070\t3246\t2648\t2252\t1887\t12101\n",
      "Model     |\t2080\t3834\t3021\t2555\t2053\t12811\n",
      "Total     |\t10703\t6847\t4962\t3844\t3066\t18255\n",
      "___________________________________________________________________\n",
      "=i B/T %     |\t19.3%\t47.4%\t53.4%\t58.6%\t61.5%\t66.3%\n",
      "=i M/T %     |\t19.4%\t56.0%\t60.9%\t66.5%\t67.0%\t70.2%\n",
      "___________________________________________________________________\n",
      ">=i B/T % |\t50.8%\t59.9%\t62.7%\t64.5%\t65.6%\t66.3%\n",
      ">=i M/T % |\t55.3%\t65.7%\t67.8%\t69.2%\t69.7%\t70.2%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('deep': conda)"
  },
  "interpreter": {
   "hash": "bace64c8930513bc16c30bbfb6191e80e6b475bb1cc35e455b666e9fcbb27152"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}